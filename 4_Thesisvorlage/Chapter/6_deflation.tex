\chapter{Deflation}
\label{sec:deflation}

% Introduction on deflation and its significance within our context of the thesis.
While evaluating the sign functions for the specific case of hermitian matrices, it is well proven that deflating the eigenvalues from the smallest could act as a catalyst to accelerate the computation \cite{10}. The reason behind utilizing this is crucial since the sign function is discontinuous at zero. This is analogous to the Non-Hermitian matrices as they have a discontinuity along the imaginary axis. A solution to the above problem would be to approximate $f$ at the eigenvalues of $A$ by a low-order polynomial. However, suppose the gap between the eigenvalues of $A$ to the left and right of the imaginary axis is too small. In that case, there exists no low-order polynomial accurate for all the eigenvalues.

Deflation introduces the idea that we dissect these critical eigenvalues from the rest and solve them exactly. Krylov subspace methods approximate the remaining deflated space. In the Hermitian case, deflation is straightforward since eigenvectors are orthonormal as mentioned in \cite{11}. For the non-Hermitian matrices, we are interested in, the (generalized) eigenvectors which are not orthonormal. For this reason, the spectral definition of the matrix functions cannot be easily decomposed into orthogonal subspaces as the definition involves the inverse of the matrix of the basis vectors. The paper\cite{11} introduces some proposals to overcome this scenario using the composite subspace generated by including a small number of critical eigenvalues in the Krylov subspace.

\section{LR-deflation}
\label{sec:LR_def}

% LR-deflation for non-Hermitian matrices.
In this approach an augmented subspace $\Omega_{m} + \mathcal{K}_{m}(A, x)$ is constructed using both left and right eigenvectors in respect to the critical eigenvalues. Here, the $m$ critical eigenvalues and the left and right eigenvectors of $A$ can be computed using appropriate iterative methods. The right eigenvectors satisfy,

\begin{equation}
    AR_{m} = R_{m}\lambda_{m}
    \label{eq:2.57}
\end{equation}

In the above equation, $\lambda_{m}$ is the diagonal eigenvalue matrix for $m$ critical eigenvalues. $R_{m} = [r_{1},\dots,r_{m}]$ the matrix of the right eigenvectors (stored as columns). The left eigenvectors satisfy,

\begin{equation}
    L_{m}^{\dagger}A = \lambda_{m}L_{m}^{\dagger}
    \label{eq:2.58}
\end{equation}

Here $L_{m} = [_{1},\dots,l_{m}]$ is the matrix containing the left eigenvectors (stored in columns). In a non-Hermitian matrix, the left and right eigenvectors corresponding to different eigenvalues are orthogonal. If there exist degenerate eigenvalues, then linear combinations of the eigenvectors can be formed in such a way that the orthogonality property remains in general valid. For normalized eigenvectors $L_{m}^{\dagger}R_{m}=I_{m}$ i.e., $l_{i}^{\dagger}r_{i}=1$. Furthermore, $R_{m}L_{m}^{\dagger}$ is an oblique projector on the subspace $\Omega_{m}$, spanned by the right eigenvectors.

Based on the above details we can now decompose the vector $x$ as,
\begin{equation}
    x = x_{\mathcal{k}} + x_{\ominus}
    \label{eq:2.59}
\end{equation}
where $x_{||} = R_{m}L_{m}^{\dagger}x$, the oblique projection of $x_{m}$ on $\Omega_{m}$ and $x_{\ominus} = x - x_{||}$. Now applying the decomposition of $x$ from the equation \ref{eq:2.59} on $f(A)$ yields,

\begin{equation}
\begin{aligned}
    f(A)x &= f(A)x_{\mathcal{k}} + f(A)x_{\ominus} \\
          &= f(A)R_{m}L_{m}^{\dagger}x + f(A)x_{\ominus}
\end{aligned}
\label{eq:2.60}
\end{equation}

Now as per the previously introduced idea, the first part of the equation \ref{eq:2.60} could be evaluated exactly using the spectral definition of the matrix functions i.e.,

\begin{equation}
    f(A)R_{m}L_{m}^{\dagger}x = R_{m}f(\lambda_{m})L_{m}^{\dagger}x
    \label{eq:2.61}
\end{equation}

The second term could be approximated with the help of some Krylov subspace approaches. This means an orthonormal basis is constructed in the Krylov subspace $\mathcal{K}_{k}(A,x_{\ominus})$. For the Arnoldi method, the subspace is created using the recurrence,
\begin{equation}
    AV_{k} = V_{k} H_{k} + \beta_{k} v_{k+1} e_{k}^{T}
    \label{eq:2.62}
\end{equation}
where, $\beta = |x_{\ominus}|$ and $v_{1} = \frac{x_{\ominus}}{\beta}$. The main advantage of such a deconstruction of $x$ to two components is that we could separate the effects of the critical eigendirections for a better approximation of the action of a matrix over a vector i.e., $\mathcal{K}_{k}(A,x_{\ominus})$ does not mix with $\Omega_{m}$. The above is summarized in the form of an algorithm as below:

\begin{algorithm}[H]
    \caption{Algorithm for approximating $f(A)x$ in the LR-deflation scheme \cite{11}}
    \label{alg:lr_deflation}
    \textbf{Given:} Matrix $A$, vector $x$, and function $f$.\\
    \textbf{Output:} Approximation of $f(A)x$.
    \begin{algorithmic}[1]
        \STATE Determine the left and right eigenvectors for $m$ critical eigenvalues of $A$ using ARPACK. Store the corresponding eigenvector matrices $L_m$ and $R_m$.
        \STATE Compute $f(\lambda_i)$ for $i = 1, \dots, m$ for the critical eigenvalues.
        \STATE Compute $x_{\Theta} = \left(1 - R_m L_m^{\dagger} \right) x$.
        \STATE Construct an orthonormal basis for the Krylov subspace $\mathcal{K}_k(A, x_\Theta)$ using the Arnoldi recurrence. The basis is constructed iteratively by orthogonalizing each new Krylov vector for all previous Arnoldi vectors and is stored as columns of a matrix $V_k$. Also, build the upper Hessenberg matrix $H_k = V_k^{\dagger} A V_k$.
        \STATE Compute the (first column of) $f(H_k)$.
        \STATE Compute the approximation to $f(A)x$ using \ref{eq:2.60}.
    \end{algorithmic}
\end{algorithm}