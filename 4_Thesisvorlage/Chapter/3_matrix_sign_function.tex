\chapter{Matrix Sign Function}
\label{sec:matrix_sign_func}

% An introduction to what a scalar sign function is!
To introduce the matrix sign function, it is essential to explore the scalar sign function as it represents an extension of their scalar counterparts. The scalar sign function is defined over the complex plane excluding the imaginary axis: $\mathbb{C}\setminus\mathbb{C}^0 = \mathbb{C}^+ \cup \mathbb{C}^-$ where $\mathbb{C}^-$, $\mathbb{C}^+$ and $\mathbb{C}^0$ denote the open right-half complex plane, the open left-half complex plane, and the imaginary axis, respectively. Thus, the scalar sign function for $z \in \mathbb{C}^+ \cup \mathbb{C}^-$ is defined by\cite{22}

\begin{equation}
    \sgn z = \begin{cases}
                1, & z \in \mathbb{C}^+ \\
                -1, & z \in \mathbb{C}^-
            \end{cases}.
            \label{eq:2.8}
\end{equation}

The above definition implies that if $z \in \mathbb{C}^{\theta} = \{iy, y \in \mathbb{R}\}$ then $\sgn(z)$ is undefined. Now based on the above, to define the matrix sign function, we proceed under the assumption that $A \in \mathbb{C}^{n \times n}$ does not possess eigenvalues lying on the imaginary axis, thereby ensuring $A$ is non-singular and $\sgn(A)$ remains well-defined. These conditions are crucial for establishing the validity of $\sgn(A)$. There exist numerous equivalents for matrix sign functions that extend meaningful insights into the properties they possess. In the following subsection, we will examine several pertinent definitions essential to this thesis.

\section{Definition of \emph{$\sgn(A)$}}
\label{sec:def_sgn(A)}

% In this subsection we will introduce the general definitions for the matrix sign function.
\begin{definition}
    \label{def:2.8}
    \cite{23}(Jordan canonical form) Let the matrix $A$ have a Jordan decomposition
    
    \[
        A=T\begin{bmatrix}
            N&0\\ 
            0&P
        \end{bmatrix}T^{-1},
    \]
    where $N$ and $P$ are square matrices with eigenvalues in $\mathbb{C}^-$ and $\mathbb{C}^+$, respectively. Then the sign of $A$ is defined to be,

    \begin{equation}
        \sgn(A)=T\begin{bmatrix}
            -I_{N}&0\\ 
            0&I_{P}
        \end{bmatrix}T^{-1},
        \label{eq:2.9}
    \end{equation}
    where the identity matrices $I_{N}$, and $I_{P}$, are compatibly dimensioned with $N$ and $P$, respectively.

\end{definition}
The above definition is a derivation of Definition \ref{def:2.3}, where the matrix function is the sign function, and for this function, all derivatives of all orders are zero.

Some intriguing properties for $\sgn(A)$ derived from the above initial definition are highlighted in the following remark.

\begin{remark}
\label{rem:2.9}
    (properties of the sign function)\cite{22, 23}
    \begin{enumerate}
        \item $\sgn(A)$ is diagonalizable with eigenvalues equal to $\pm1$.
        \item $\sgn(A)^2 = I$.
        \item  If $c$ is a nonzero real scalar, then $\sgn(cA)=\sgn(c)\sgn( A)$.
        \item $\nega(A) \equiv (I-\sgn(A))/2$ is a projection onto the negative invariant subspace of $A$ and $\posi(A) \equiv (I+\sgn(A))/2$ is a projection onto the positive invariant subspace of $A$, where the positive and negative invariant subspaces of $A$ are the subspaces corresponding to the eigenvalues of $A$ in $\mathbb{C}^-$ and $\mathbb{C}^+$ respectively.
    \end{enumerate}
\end{remark}

\begin{lemma}
    \label{lem:2.10}
    \cite{22}Given,
    
    \[
        A=U\begin{bmatrix}
            N&T\\ 
            0&P
        \end{bmatrix}U^{T},
    \]
    where $U$ is an orthogonal matrix, $N$ has eigenvalues in $\mathbb{C}^-$, and $P$ has eigenvalues in $\mathbb{C}^+$. Then the sign of $A$ is given by,

   \begin{equation}
        sgn(A)=U\begin{bmatrix}
            -I_{N}&S\\ 
            0&I_{P}
        \end{bmatrix}U^{T},
        \label{eq:2.10}
   \end{equation}
    where $S$ satisfies the Sylvester equation,
    \begin{equation}
        NS - SP = -2T.
        \label{eq:2.11}
    \end{equation}

\end{lemma}

The above formulation proves to be highly beneficial. It can be used to analyze the stability of Newton iteration \cite{24, 25} and analyze the conditioning \cite{25} of the matrix sign function. This definition further serves as a foundation for a method of solving the stable Sylvester equation of the form \eqref{eq:2.11}. In the equation \eqref{eq:2.10} replace $U$ with $I$ to determine $\sgn(A)$. Now, the upper right block which is $S$ of $\sgn(A)$ is the solution desired to be found from the above-stated Sylvester equation.

The second type of definition is based on integral representations. Utilizing a residue argument, presented in the spectral theory of operators, Robert derives an integral formula of the form \cite{23},

\begin{equation}
    \posi(A) = \frac{1}{2 \pi i} \int_{D} (\zeta I - A)^{-1} d\zeta,
    \label{eq:2.12}
\end{equation}
where $D$ is a simple closed contour in $\mathbb{C}^{+}$ containing the eigenvalues of $A$ with positive real part and pos(A) as mentioned in remark \ref{rem:2.9}. From this equation and the remark \ref{rem:2.9} Robert derived an integral representation as a definition for $\sgn(A)$,

\begin{definition}
    \label{def:2.11}
   \begin{equation}
        \sgn(z) = \frac{2}{\pi}z\int_{0}^{+\infty}(y^{2}I+z^{2})^{-1}dy.
        \label{eq:2.13}
    \end{equation} 
\end{definition}
    
Definition \ref{def:2.7} and the above definition are identical when $f(z) \equiv 1$ for the contour $\mathcal{C}^+$ in Definition \ref{def:2.7}. 

The third method of defining $\sgn(A)$ is through matrix iterations. Newton's iteration is the most popular iterative method to find $\sgn(A)$. The method is applied to the equation $S^{2}-I=0$. Let $A_{0} = A_k$ and set

\begin{equation}
    A_{k+1}=\frac{1}{2}(A_{k}+A_{k}^{-1}).
    \label{eq:2.15}
\end{equation}

The above matrix iteration is globally convergent for all matrices $A$ with eigenvalues in $\mathbb{C}^{-} \cup \mathbb{C}^{+}$.

\begin{equation}
    sgn(A) = \lim_{k\longrightarrow+\infty}A_{k}.
    \label{eq:2.16}
\end{equation}

An intriguing aspect of Newton's iterative method is that the convergence is quadratic when $A_k$ is close to the actual $\sgn(A)$ but could be relatively slow at the initial stages.

Higher-order Padé iterative methods are another form of iterative methods used for the computation of $\sgn(A)$. The general form of the equation used in these iterations for order $n$ is,

\begin{equation}
    A_{k+l} = P_{n}(A_{k})Q_{n}^{-1}(A_{k}),
    \label{eq:2.17}
\end{equation}
where $P_{n}(A)$ and $Q_{n}(A)$ are the odd and even parts respectively of the polynomial $(I+A)^{n}$.The Padé iterations are globally convergent and serve as an implicit definition for $\sgn(A)$. Introduction of the $\tanh$ identity in equation \eqref{eq:2.17} helps in the study of chaotic behaviours of $\sgn(A)$ on the eigenvalues of $A$ close to the imaginary axis \cite{26}.

\begin{equation}
    P_{n}(A_{k})Q_{n}^{-1}(A_{k}) = \tanh(n\arctanh(A_{k})).
    \label{eq:2.18}
\end{equation}

If we represent $x$ in polar form, i.e., $x = r e^{i \phi}$, then $x$ has two principal branches, given by $\sqrt{x} = \pm\sqrt{r} e^{i \frac{\phi}{2}}$ for $\phi \in [-\pi, \pi]$. Following the first type of definition and extending the scalar formulation of the sign function, we define $\sgn(z) = \frac{z}{\sqrt{z^2}}$ as presented by Higham \cite{27} and apply it to the corresponding matrix function. This extension holds only when $z$ is not purely imaginary and consequently, when extended to a matrix $A$, the matrix must have no purely imaginary eigenvalues.

For such matrices, $A^2$ contains no eigenvalues on the negative real axis, thus ensuring that there exists a unique square root, $N=(A^{2})^{\frac{1}{2}}$. This commutes with $A$ and has eigenvalues in the open right-half complex plane as cited in the paper \cite{28}. Thus we have a definition for the matrix sign function:

\begin{definition}
    \label{def:2.12}
    Let $spec(A) \cap \mathbb{R}^{-} = \emptyset$, then 
    \begin{equation}
        \sgn(A) = A(A^2)^{-\frac{1}{2}}.
        \label{eq:2.19}
    \end{equation}
\end{definition}