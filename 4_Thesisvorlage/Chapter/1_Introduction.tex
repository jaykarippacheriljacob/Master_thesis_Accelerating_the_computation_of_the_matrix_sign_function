\chapter{Introduction}
\label{sec: intro}

% Introduction to matrix functions and their applications.
Consider a matrix $\mathbf{A} \in \mathbb{C}^{n \times n}$, a vector $\mathbf{b} \in \mathbb{C}^{n}$ and a function $\mathbf{f} : \mathbb{C} \to \mathbb{C}$, the action of a matrix is defined as:
\begin{equation}
    f(A)b
    \label{eq:1.1}
\end{equation}
The above expression represents a product of the matrix function $f(A) \in \mathbb{C}^{n \times n}$ on a vector \textbf{b}. There exists a huge interest in the action of a matrix on a vector in the fields of science and engineering.
Some of the most interesting cases widely under studies are :
\begin{enumerate}
    \item \textbf{Matrix exponential function}  $f(z) = e^z$, forms the core of exponential integrators used for solving differential equations \cite{1,2,3}. 
    \item \textbf{Matrix square root} $f(z) = z^{1/2}$, in machine learning \cite{6} and in other domains such as image processing, advection-diffusion problems, elasticity and many more \cite{4,5}.
    \item \textbf{Matrix logarithm} $f(z) = \log(z)$, used in Markov model analysis \cite{7}.
    \item \textbf{Matrix fractional powers} $f(z) = z^\alpha$, in fractional differential equations \cite{9}.
    \item \textbf{Matrix sign function} $f(z) = \sgn(z)$, in lattice quantum chromodynamics (QCD) \cite{11, 10}.
\end{enumerate}
The most straightforward approach to compute $f(A)\mathbf{b}$ is to first calculate $f(A)$ and then perform matrix multiplication with $\mathbf{b}$. However, as the dimension of the matrix grows, this approach becomes impractical due to various reasons such as the storage complexity, computational cost of matrix functions, and inefficiency of matrix-vector multiplication.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Add a sentence explaining the reasons.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Brief QCD lattice computation and the type of matrices we are dealing with.
Here, our domain of interest is the matrix sign function, in conjunction with the application of lattice QCD. The most significant challenge faced in lattice QCD was the implementation of chiral symmetry on the lattice \cite{12} and one among the prominent solutions proposed to overcome this was the Overlap-Dirac operator involving the sign function, which avoids low mode calculation for chiral symmetry \cite{13}. However, the drawback of the above proposal was the huge computational cost of the matrix sign function since the matrix $A$ is a large sparse matrix. Typically the matrix $A$ is Hermitian and efficient methods have been already developed to approximate them as mentioned in papers \cite{14,10}.

Studying the relativistic heavy ion collisions theoretically in lattice simulations and model calculations implies presenting a non-zero density. As a result, a quark chemical potential is introduced to the QCD Lagrangian, leading to the loss of hermiticity of the matrix $A$ as in \cite{16}. Therefore, we are now faced with the computation of the explained matrix sign function for a non-Hermitian matrix $A$. Furthermore, it is to be highlighted that we will always consider the inverse square root function, since $\text{sign}(Q)x = (Q^2)^{-1/2}Qx$, which would be further detailed in the following chapters.

% How to solve these kinds of problems and the problems present in them
For smaller lattices, the existing methods could be used in the above-mentioned problem. However, as the dimension of the matrix becomes larger, one has to heavily depend upon iterative methods for approximating the matrix sign function. Some of the popular methods under use in such situations are polynomial \cite{17,18} and rational \cite{19,20,21} Krylov methods which demand a high arithmetic cost for the orthogonalization of a Krylov basis or a large memory cost for the storage of Krylov basis vectors. These limitations narrow down the attainable accuracy of the Krylov methods. To address these constraints, there are several strategies available. Among them a few strategy of interest that could accelerate the convergence are:
\begin{enumerate}
    \item \textbf{Restarts} (in the non-Hermitian case). This avoids having too many inner products in the Arnoldi orthogonalization \cite{52}.
    \item \textbf{Deflation} (explicit and implicit). This makes the matrix better conditioned and thus reduces the number of iterations. Explicit deflation uses the smallest left and right eigenvectors; \cite{52, 11}.
    \item \textbf{Polynomial preconditioning.} This also makes the matrix better conditioned and thus reduces the number of iterations \cite{49}.
    \item \textbf{Sketching.} This is a randomized approach where we save orthogonalizations and sketch the Arnoldi matrix \cite{41}.
\end{enumerate}

% Brief on what is being discussed and analysed in this thesis
In this thesis, we explore new possibilities arising from the combination of deflation and Krylov subspace methods based on the above strategies, chosen for their strengths in relation to the matrix sign function and specific applications of interest. In Chapter \ref{sec:matrix_fun}, we begin with an introduction to matrix functions, including essential definitions and properties. This discussion narrows in Chapter \ref{sec:matrix_sign_func}, where we focus on the matrix sign function, our primary area of investigation. Here, we cover definitions and properties derived from matrix functions, along with specific characteristics unique to the matrix sign function.

We indicated that our interest are in large non-Hermitian matrices of dimension $N$. Hence, in chapter \ref{sec:QCD_sim_and_non_herm_challenges} provides a concise overview of Quantum Chromodynamics (QCD) simulations, particularly the Wilson-Dirac and overlap operators in lattice QCD. We examine the limitations encountered in calculating sign functions in this context, highlighting the challenges and the motivation to develop algorithms that enhance efficiency and stability.

Our goal is to approximate the action of a matrix sign function on a vector more efficiently and stably. To this end, we introduce recent methods identified in our literature review, alongside algorithms for their implementation, in Chapters \ref{sec:kryl_subspace_app} and \ref{sec:deflation}. In Chapter \ref{sec:expl_poss}, we present a framework for implementing potential new algorithms and discuss the rationale behind the choices and combinations selected for our numerical experiments. Finally, Chapter \ref{sec:num_exper} provides an in-depth analysis of the performance of these algorithms across various parameters.